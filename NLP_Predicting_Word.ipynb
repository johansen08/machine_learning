{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Friday, May 30th 2024"
      ],
      "metadata": {
        "id": "e8FT8-DW8lre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import Library"
      ],
      "metadata": {
        "id": "dd7IU7Pt9xzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8k8cChAn7CUB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Install gdown\n",
        "---\n",
        "Gdown adalah alat yang berguna untuk mengunduh file dari Google Drive\n",
        "menggunakan terminal atau command line interface (CLI)"
      ],
      "metadata": {
        "id": "-7iO_enU91wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown==5.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zepiOICr98ff",
        "outputId": "b1b77276-a5a7-4149-9c26-88cfd0dc6f2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown==5.1.0 in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown==5.1.0) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==5.1.0) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==5.1.0) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Download file yang akan digunakan sebagai dataset"
      ],
      "metadata": {
        "id": "DaBwUE09-eq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNeM40nK-oJH",
        "outputId": "dd1c9224-e2f9-474e-97d4-95602dbd037b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 133MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Read file"
      ],
      "metadata": {
        "id": "PAwFN5D7-5aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = './sonnets.txt'\n",
        "\n",
        "with open(FILE) as f:\n",
        "  ds = f.read()"
      ],
      "metadata": {
        "id": "PinW0AYk_NAC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Convert ke huruf kecil dan bagi menjadi list"
      ],
      "metadata": {
        "id": "TtFQqxV6_lxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.lower().split('\\n')"
      ],
      "metadata": {
        "id": "iWEjf7ZH_wCD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds))\n",
        "print(ds[0])\n",
        "print(ds[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKDRRAKIADow",
        "outputId": "bb29f6f3-ce1c-4be6-cdb1-2126f027d402"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2159\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Ubah setiap kata menjadi indeks menggunakan Tokenizer\n",
        "---\n",
        "Tokenizer digunakan untuk mengubah teks menjadi urutan token atau indeks.\n"
      ],
      "metadata": {
        "id": "U2TVshe3Ao5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(ds)"
      ],
      "metadata": {
        "id": "tD0yz7dBAMEV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(tokenizer.word_index) + 1 # tambah 1 karena index biasanya dimulai dari 0"
      ],
      "metadata": {
        "id": "66aKZEruBM0h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_VYXfA1_xzC",
        "outputId": "61249cc6-59de-4ea4-85b5-267dca1a8209"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Change text to sequences"
      ],
      "metadata": {
        "id": "DHEsfaK0C2Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WARNING : DON'T PASS STRING TO THE FUNCTION, MUST BE A LIST\n",
        "print(ds[0])\n",
        "tokenizer.texts_to_sequences(ds[0]) #THIS IS WRONG, text_to_sequences receive list\n",
        "#ds[0] CONTAIN A STRING WHICH CAN ITERATE AS A CHARACTER\n",
        "#WHEN YOU PASS ds to fit_on_texts() IT CHANGE WORD TO AN INDEX SO YOU SHOULD PASS A LIST OF STRINGS\n",
        "\n",
        "tokenizer.texts_to_sequences([ds[0]]) #RIGHT WAY TO DO\n",
        "tokenizer.texts_to_sequences([ds[0]])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yT4PAVsJC9S5",
        "outputId": "c1934d54-e9f5-4a8a-a35a-3363b207ea0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fairest creatures we desire increase,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram_seqs(ds, tokenizer):\n",
        "  input_sequences = [] #consist all list that already converted to index and all the subsequence of that list\n",
        "  for row in ds:\n",
        "    row_in_token = tokenizer.texts_to_sequences([row])[0] #be careful in this part should pass in list shape and only take the inner value from [[a]]\n",
        "    for i in range(1, len(row_in_token)):\n",
        "      n_seq = row_in_token[:i+1]\n",
        "      input_sequences.append(n_seq)\n",
        "  return input_sequences"
      ],
      "metadata": {
        "id": "jQShjonmEYEM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = n_gram_seqs(ds, tokenizer)\n",
        "max_seq_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "tQEhufarIQvj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_sequences))\n",
        "print(max_seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbVRkh8IIvLg",
        "outputId": "43316d04-2ef7-4732-d363-89e8e5dc443e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15462\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Add padding to sequences\n",
        "---\n",
        "add padding to all list make all the list have the same length and we make max_seq_len as the max len and a list lower than that number should add padding 0 to the left empty part"
      ],
      "metadata": {
        "id": "ehjd6Z7SI5nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seqs(input_sequences, max_seq_len):\n",
        "  padded_seqs = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "  return padded_seqs"
      ],
      "metadata": {
        "id": "vfhi84KZJY5c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seqs = pad_seqs(input_sequences, max_seq_len)"
      ],
      "metadata": {
        "id": "1lfH_IMb3vHA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"padded dataset has shape: {padded_input_seqs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2JeRu3Q4J4B",
        "outputId": "1b08b434-3cce-40f6-d8f5-d4506d930dfd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded dataset has shape: (15462, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Split Data into features and labels"
      ],
      "metadata": {
        "id": "88MzmNrB4Yi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features_and_labels(seq, total_words):\n",
        "  features = seq[:, :-1]\n",
        "  labels = seq[:, -1]\n",
        "  one_hot_labels = to_categorical(labels, num_classes=total_words, dtype='int8')\n",
        "  return features, one_hot_labels"
      ],
      "metadata": {
        "id": "7WDElCsu4dtX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = features_and_labels(padded_input_seqs, total_words)"
      ],
      "metadata": {
        "id": "FIPLXwyD5bIY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Create Model"
      ],
      "metadata": {
        "id": "fcv2E5yN6vkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOMETHING CAN IMPROVE:\n",
        "*   OUTPUT DIM : instead using 100 might be another option\n",
        "*   LSTM UNITS : instead using 100 might be anothe option\n",
        "\n"
      ],
      "metadata": {
        "id": "RAuHtvvsHLU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(total_words, input_len):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 100, input_length=input_len))\n",
        "  model.add(Bidirectional(LSTM(100)))\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "  model.compile(\n",
        "      loss='categorical_crossentropy',\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "5fL56Q1u6rO1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model(total_words, max_seq_len-1)"
      ],
      "metadata": {
        "id": "nmGInvcIFgD4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(features, labels, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHCNbQQuGwb-",
        "outputId": "e449ed01-06c0-4883-d040-ac6ee2c13012"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "484/484 [==============================] - 17s 25ms/step - loss: 6.8824 - accuracy: 0.0229\n",
            "Epoch 2/50\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 6.4459 - accuracy: 0.0314\n",
            "Epoch 3/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 6.2224 - accuracy: 0.0413\n",
            "Epoch 4/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 5.9728 - accuracy: 0.0521\n",
            "Epoch 5/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.7053 - accuracy: 0.0616\n",
            "Epoch 6/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 5.4202 - accuracy: 0.0726\n",
            "Epoch 7/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 5.1120 - accuracy: 0.0856\n",
            "Epoch 8/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.8001 - accuracy: 0.1062\n",
            "Epoch 9/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 4.4979 - accuracy: 0.1263\n",
            "Epoch 10/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 4.1990 - accuracy: 0.1640\n",
            "Epoch 11/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 3.9112 - accuracy: 0.2086\n",
            "Epoch 12/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.6396 - accuracy: 0.2561\n",
            "Epoch 13/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 3.3818 - accuracy: 0.3014\n",
            "Epoch 14/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 3.1426 - accuracy: 0.3481\n",
            "Epoch 15/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.9233 - accuracy: 0.3890\n",
            "Epoch 16/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 2.7164 - accuracy: 0.4343\n",
            "Epoch 17/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.5371 - accuracy: 0.4683\n",
            "Epoch 18/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.3714 - accuracy: 0.4968\n",
            "Epoch 19/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.2121 - accuracy: 0.5325\n",
            "Epoch 20/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 2.0735 - accuracy: 0.5586\n",
            "Epoch 21/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.9350 - accuracy: 0.5904\n",
            "Epoch 22/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 1.8178 - accuracy: 0.6157\n",
            "Epoch 23/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 1.7067 - accuracy: 0.6402\n",
            "Epoch 24/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5987 - accuracy: 0.6667\n",
            "Epoch 25/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.5005 - accuracy: 0.6852\n",
            "Epoch 26/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.4116 - accuracy: 0.7051\n",
            "Epoch 27/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3293 - accuracy: 0.7240\n",
            "Epoch 28/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.2534 - accuracy: 0.7407\n",
            "Epoch 29/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 1.1796 - accuracy: 0.7553\n",
            "Epoch 30/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 1.1168 - accuracy: 0.7689\n",
            "Epoch 31/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0539 - accuracy: 0.7821\n",
            "Epoch 32/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 1.0012 - accuracy: 0.7908\n",
            "Epoch 33/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.9686 - accuracy: 0.7969\n",
            "Epoch 34/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9145 - accuracy: 0.8074\n",
            "Epoch 35/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.8711 - accuracy: 0.8150\n",
            "Epoch 36/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.8335 - accuracy: 0.8236\n",
            "Epoch 37/50\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8109 - accuracy: 0.8271\n",
            "Epoch 38/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.7822 - accuracy: 0.8297\n",
            "Epoch 39/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.7509 - accuracy: 0.8353\n",
            "Epoch 40/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.7327 - accuracy: 0.8361\n",
            "Epoch 41/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.7125 - accuracy: 0.8403\n",
            "Epoch 42/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 0.6884 - accuracy: 0.8417\n",
            "Epoch 43/50\n",
            "484/484 [==============================] - 4s 7ms/step - loss: 0.6783 - accuracy: 0.8430\n",
            "Epoch 44/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.6722 - accuracy: 0.8427\n",
            "Epoch 45/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 0.6542 - accuracy: 0.8436\n",
            "Epoch 46/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 0.6396 - accuracy: 0.8464\n",
            "Epoch 47/50\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.6310 - accuracy: 0.8463\n",
            "Epoch 48/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 0.6176 - accuracy: 0.8490\n",
            "Epoch 49/50\n",
            "484/484 [==============================] - 3s 7ms/step - loss: 0.6135 - accuracy: 0.8457\n",
            "Epoch 50/50\n",
            "484/484 [==============================] - 4s 8ms/step - loss: 0.6067 - accuracy: 0.8479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRY TO PREDICT"
      ],
      "metadata": {
        "id": "J7pexEccH-Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I like\"\n",
        "next_words = 10\n",
        "for _ in range(next_words):\n",
        "    # Convert the text into sequences\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    # Pad the sequences\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    # Get the probabilities of predicting a word\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    # Choose the next word based on the maximum probability\n",
        "    predicted = np.argmax(predicted, axis=-1).item()\n",
        "    # Get the actual word from the word index\n",
        "    output_word = tokenizer.index_word[predicted]\n",
        "    # Append to the current text\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8kGnIsEIA5J",
        "outputId": "06a33e81-0908-4f17-81b0-dc8b3ed4cb33"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to be fair more than my friend can five tend\n"
          ]
        }
      ]
    }
  ]
}